---
title: "hw-03-02-eshaanmathakari"
author: "Eshaan Mathakari"
format: html
editor: visual
---
# Classification: Alternative Techniques

## Install packages
```{r}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(
  C50,                # C5.0 Decision Trees and Rule-Based Models
  caret,              # Classification and Regression Training
  e1071,              # Misc Functions of the Department of Statistics (e1071), TU Wien
  keras,              # R Interface to 'Keras'
  kernlab,            # Kernel-Based Machine Learning Lab
  lattice,            # Trellis Graphics for R
  MASS,               # Support Functions and Datasets for Venables and Ripley's MASS
  mlbench,            # Machine Learning Benchmark Problems
  nnet,               # Feedforward Neural Networks and Multinomial Log-Linear Models
  palmerpenguins,     # Palmer Archipelago (Antarctica) Penguin Data
  party,              # A Laboratory for Recursive Partytioning
  partykit,           # A Toolkit for Recursive Partytioning
  randomForest,       # Breiman and Cutler's Random Forests for Classification and Regression
  rpart,              # Recursive partitioning models
  RWeka,              # R/Weka Interface
  scales,             # Scale Functions for Visualization
  tidymodels,         # Tidy machine learning framework
  tidyverse,          # Tidy data wrangling and visualization
  xgboost             # Extreme Gradient Boosting
)
```

Show fewer digits

```{r}
options(digits=3)
```

## **Training and Test Data**

We will continue using artists data set:

```{r}
artists <- as.data.frame(artists)
artists |> glimpse()
```

Test data is not used in the model building process and needs to be set aside purely for testing the model after it is completely built. Here I use 80% for training.

```{r}
set.seed(123)  # for reproducibility
inTrain <- createDataPartition(y = artists$book, p = .8)[[1]]
artists_train <- dplyr::slice(artists, inTrain)
artists_test <- dplyr::slice(artists, -inTrain)
```

## **Fitting Different Classification Models to the Training Data**

```{r}
train_index <- createFolds(artists_train$book, k = 10)
```

### **Conditional Inference Tree (Decision Tree)**

Conditional Inference Trees is a different kind of decision tree that uses recursive partitioning of dependent variables based on the value of correlations.

```{r}
ctreeFit <- artists_train |> train(book ~ .,
  method = "ctree",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
ctreeFit
```

```{r}
plot(ctreeFit$finalModel)
```

The conditional inference tree model performed very well in terms of accuracy and kappa, and the hyperparameter "mincriterion" was set to 0.99 to achieve this level of performance.

### **C 4.5 Decision Tree**

```{r}
# Train a C4.5-like decision tree (J48) classification model
C45Fit <- artists_train |> train(book ~ .,
  method = "J48",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
C45Fit
```

The model with the highest Accuracy value was selected as the optimal model.The final values used for the model were **C = 0.01** and **M = 1**.

```{r}
C45Fit$finalModel
```

### **K-Nearest Neighbors**

```{r warning=FALSE}
# Train a k-Nearest Neighbors (k-NN) classification model
knnFit <- artists_train |> train(book ~ .,
  method = "knn",
  data = _,
  preProcess = "scale",
    tuneLength = 5,
  tuneGrid=data.frame(k = 1:10),
    trControl = trainControl(method = "cv", indexOut = train_index))
knnFit
```

```{r}
knnFit$finalModel
```

### **PART (Rule-based classifier)**

PART stands for "Partial C 4.5," and it is an extension of the well-known C4.5 decision tree algorithm. PART is designed for classification tasks and is particularly useful when dealing with datasets that contain missing values or noisy data.

```{r}
# Train a rule-based classification model using the PART algorithm
rulesFit <- artists_train |> train(book ~ .,
  method = "PART",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index))
rulesFit
```

```{r}
rulesFit$finalModel
```

### **Linear Support Vector Machines**

A Linear Support Vector Machine (Linear SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is a variant of the traditional Support Vector Machine (SVM) algorithm that focuses on finding a linear decision boundary to separate data into different classes.

```{r warning=FALSE}
# Train a Support Vector Machine (SVM) classification model with a linear kernel
svmFit <- artists_train |> train(book ~.,
  method = "svmLinear",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
svmFit
```

```{r}
svmFit$finalModel
```

### **Random Forest**

Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.

```{r}
# Train a Random Forest classification model
randomForestFit <- artists_train |> train(book ~ .,
  method = "rf",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
randomForestFit
```

```{r}
randomForestFit$finalModel
```

### **Gradient Boosted Decision Trees (xgboost)**

Gradient-boosted decision trees are a popular method for solving prediction problems in both classification and regression domains. The approach improves the learning process by simplifying the objective and reducing the number of iterations to get to a sufficiently optimal solution.

```{r}
# Train a classification model using the XGBoost algorithm
xgboostFit <- artists_train |> train(book ~ .,
  method = "xgbTree",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index),
  tuneGrid = expand.grid(
    nrounds = 20,
    max_depth = 3,
    colsample_bytree = .6,
    eta = 0.1,
    gamma=0,
    min_child_weight = 1,
    subsample = .5
  ))
xgboostFit
```

```{r}
xgboostFit$finalModel
```

### **Artificial Neural Network**

```{r}
nnetFit <- artists_train |> train(book ~ .,
  method = "nnet",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index),
  trace = FALSE)
nnetFit
```

```{r}
nnetFit$finalModel
```

## **Comparing Models**

Collect the performance metrics from the models trained on the same data.

```{r}
resamps <- resamples(list(
  ctree = ctreeFit,
  C45 = C45Fit,
  SVM = svmFit,
  KNN = knnFit,
  rules = rulesFit,
  randomForest = randomForestFit,
  xgboost = xgboostFit,
  NeuralNet = nnetFit
    ))
resamps
```

```{r}
summary(resamps)
```

```{r}
library(lattice)
bwplot(resamps, layout = c(3, 1))
```

For the "CART" model, the accuracy values range from a minimum of 1.000 to a maximum of 1.000, indicating high performance.

For the "kNearestNeighbors" (k-NN) model, the accuracy values are slightly lower, with 0.961, which means the k-NN model is slightly less accurate on average compared to CART.

```{r}
difs <- diff(resamps)
difs
```

```{r}
summary(difs)
```

The small p-values for both Accuracy and Kappa comparisons mean that there is a significant difference in performance between the CART and k-NN models.

## **Applying the Chosen Model to the Test Data**

```{r}
pr <- predict(xgboostFit, artists_test)
```

Calculate the confusion matrix for the held-out test data.

```{r}
confusionMatrix(pr, reference = artists_test$book)
```

## **Comparing Decision Boundaries of Popular Classification Techniques**

Classifiers create decision boundaries to discriminate between classes. Different classifiers are able to create different shapes of decision boundaries (e.g., some are strictly linear) and thus some classifiers may perform better for certain datasets.

```{r}
library(scales)
library(tidyverse)
library(ggplot2)
library(caret)

decisionplot <- function(model, data, class_var, 
  predict_type = c("class", "prob"), resolution = 3 * 72) {
  # resolution is set to 72 dpi if the image is rendered  3 inches wide. 
  
  y <- data |> pull(class_var)
  x <- data |> dplyr::select(-all_of(class_var))
  
  # resubstitution accuracy
  prediction <- predict(model, x, type = predict_type[1])
  # LDA returns a list
  if(is.list(prediction)) prediction <- prediction$class
  prediction <- factor(prediction, levels = levels(y))
  
  cm <- confusionMatrix(data = prediction, 
                        reference = y)
  acc <- cm$overall["Accuracy"]
  
  # evaluate model on a grid
  r <- sapply(x[, 1:2], range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as_tibble(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  cl <- predict(model, g, type = predict_type[1])
  
  # LDA returns a list
  prob <- NULL
  if(is.list(cl)) { 
    prob <- cl$posterior
    cl <- cl$class
  } else
    if(!is.na(predict_type[2]))
      try(prob <- predict(model, g, type = predict_type[2]))
  
  # we visualize the difference in probability/score between the 
  # winning class and the second best class.
  # don't use probability if predict for the classifier does not support it.
  max_prob <- 1
  if(!is.null(prob))
    try({
      max_prob <- t(apply(prob, MARGIN = 1, sort, decreasing = TRUE))
      max_prob <- max_prob[,1] - max_prob[,2]
    }, silent = TRUE) 
  
  cl <- factor(cl, levels = levels(y))
  
  g <- g |> add_column(prediction = cl, probability = max_prob)
  
  ggplot(g, mapping = aes(
    x = .data[[colnames(g)[1]]], y = .data[[colnames(g)[2]]])) +
    geom_raster(mapping = aes(fill = prediction, alpha = probability)) +
    geom_contour(mapping = aes(z = as.numeric(prediction)), 
      bins = length(levels(cl)), linewidth = .5, color = "black") +
    geom_point(data = data, mapping =  aes(
      x = .data[[colnames(data)[1]]], 
      y = .data[[colnames(data)[2]]],
      shape = .data[[class_var]]), alpha = .7) + 
    scale_alpha_continuous(range = c(0,1), limits = c(0,1), guide = "none") +  
    labs(subtitle = paste("Training accuracy:", round(acc, 2))) +
     theme_minimal(base_size = 14)
}
```

### **The Artist Dataset**

We will continue using same dataset and will be using edition_number, year and book as parameters.

```{r}
### Three classes 
### (note: MASS also has a select function which hides dplyr's select)
x <- artists |> dplyr::select(edition_number, year, book)
```

```{r}
ggplot(x, aes(x = edition_number, y = year, fill = book)) +  
  stat_density_2d(geom = "polygon", aes(alpha = after_stat(level))) +
  geom_point() +
  theme_minimal(base_size = 14) +
  labs(x = "Edition number",
       y = "Year",
       fill = "Book",
       alpha = "Density")
```

#### K-Nearest Neighbors Classifier

set the number of nearest neighbors to 1

```{r}
model <- x |> caret::knn3(book ~ ., data = _, k = 1)
decisionplot(model, x, class_var = "book") + 
  labs(title = "kNN (1 neighbor)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

set the number of nearest neighbors to 3

```{r}
model <- x |> caret::knn3(book ~ ., data = _, k = 3)
decisionplot(model, x, class_var = "book") + 
  labs(title = "kNN (3 neighbor)",
        x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

set the number of nearest neighbors to 9

```{r}
model <- x |> caret::knn3(book ~ ., data = _, k = 9)
decisionplot(model, x, class_var = "book") + 
  labs(title = "kNN (3 neighbor)",
        x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

#### Naive Bayes Classifier

```{r}
model <- x |> e1071::naiveBayes(book ~ ., data = _)
decisionplot(model, x, class_var = "book", 
             predict_type = c("class", "raw")) + 
  labs(title = "Naive Bayes",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction") 
```

#### Linear Discriminant Analysis

Linear Discriminant Analysis (LDA) is a supervised learning algorithm used for classification tasks in machine learning. It is a technique used to find a linear combination of features that best separates the classes in a dataset.

```{r}
model <- x |> MASS::lda(book ~ ., data = _)
decisionplot(model, x, class_var = "book") + 
  labs(title = "LDA",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

#### Multinomial Logistic Regression (implemented in nnet)

Multinomial logistic regression is an extension of logistic regression to problems with more than two classes.

```{r}
model <- x |> nnet::multinom(book ~., data = _)
```

```{r}
decisionplot(model, x, class_var = "book") + 
  labs(title = "Multinomial Logistic Regression",
      x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

#### Decision Trees

```{r}
model <- x |> rpart::rpart(book ~ ., data = _)
decisionplot(model, x, class_var = "book") + 
  labs(title = "CART",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

**`cp`** (complexity parameter) is set to 0.001, which controls the trade-off between model complexity and accuracy. A lower **`cp`** value results in a more complex tree. **`minsplit`** sets the minimum number of observations required to split a node.

```{r}
model <- x |> rpart::rpart(book ~ ., data = _,
  control = rpart.control(cp = 0.001, minsplit = 1))
decisionplot(model, x, class_var = "book") + 
  labs(title = "CART (overfitting)",
       x = "Edition number)",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <- x |> C50::C5.0(book ~ ., data = _)
decisionplot(model, x, class_var = "book") + 
  labs(title = "C5.0",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <- x |> randomForest::randomForest(book ~ ., data = _)
decisionplot(model, x, class_var = "book") + 
  labs(title = "Random Forest",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

#### SVM

SVM algorithms are very effective as we try to find the maximum separating hyperplane between the different classes available in the target feature.

```{r}
model <- x |> e1071::svm(book ~ ., data = _, kernel = "linear")
decisionplot(model, x, class_var = "book") + 
  labs(title = "SVM (linear kernel)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <- x |> e1071::svm(book ~ ., data = _, kernel = "radial")
decisionplot(model, x, class_var = "book") + 
  labs(title = "SVM (radial kernel)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <- x |> e1071::svm(book ~ ., data = _, kernel = "polynomial")
decisionplot(model, x, class_var = "book") + 
  labs(title = "SVM (polynomial kernel)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <- x |> e1071::svm(book ~ ., data = _, kernel = "sigmoid")
decisionplot(model, x, class_var = "book") + 
  labs(title = "SVM (sigmoid kernel)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

#### Single Layer Feed-forward Neural Networks

```{r}
model <-x |> nnet::nnet(book ~ ., data = _, size = 1, trace = FALSE)
decisionplot(model, x, class_var  = "book", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (1 neuron)",
       x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <-x |> nnet::nnet(book ~ ., data = _, size = 2, trace = FALSE)
decisionplot(model, x, class_var  = "book", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (2 neurons)",
        x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

```{r}
model <-x |> nnet::nnet(book ~ ., data = _, size = 4, trace = FALSE)
decisionplot(model, x, class_var  = "book", 
  predict_type = c("class", "raw")) + 
  labs(title = "NN (4 neurons)",
      x = "Edition number",
       y = "Year",
       shape = "Book",
       fill = "Prediction")
```

**`size`** specifies that the neural network should have how many hidden neurons in a single hidden layer.